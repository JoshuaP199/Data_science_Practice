{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nA Model is a specification of a mathematical (or probabilistic) relationship that exists between different variables. \\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#Modeling\n",
    "'''\n",
    "A Model is a specification of a mathematical (or probabilistic) relationship that exists between different variables. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nCreating and using models that are learned from data.\\n\\npredictive modeling or data mining.\\n\\nOur goal will be to use existing data to develop models that we can use ot predict various outcomes for new data, such as:\\n    -Whether an email message is spam or not\\n    -Whether a credit card transaction is fraudulent\\n    -Which advertisement a shopper is most likely to click on\\n    -Which football team is going to win the Super Bowl\\n\\nSupervised models - in which there is a set of data labeled     with the correct answers to learn from\\nUnsupervised models - in which there are no such labels\\nSemisupervised models - in which only some of the data are      labeled\\nOnline models - in which the model needs to continuously        adjust to newly arriving data\\nReinforcement models - in which, after making a series of       predictions, the model gets a signal indicating how well     it did\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#What is Machine Learning?\n",
    "'''\n",
    "Creating and using models that are learned from data.\n",
    "\n",
    "predictive modeling or data mining.\n",
    "\n",
    "Our goal will be to use existing data to develop models that we can use ot predict various outcomes for new data, such as:\n",
    "    -Whether an email message is spam or not\n",
    "    -Whether a credit card transaction is fraudulent\n",
    "    -Which advertisement a shopper is most likely to click on\n",
    "    -Which football team is going to win the Super Bowl\n",
    "\n",
    "Supervised models - in which there is a set of data labeled with the correct answers to learn from\n",
    "Unsupervised models - in which there are no such labels\n",
    "Semisupervised models - in which only some of the data are labeled\n",
    "Online models - in which the model needs to continuously adjust to newly arriving data\n",
    "Reinforcement models - in which, after making a series of predictions, the model gets a signal indicating how well it did\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nA common danger in ML is overfitting - producing a model that performs well on the data you train it on but generalizes poorly to any new data\\nThe other side of this is underfitting - producing a model that doesn't perform well even on the training data, although typically when this happens you decide your model isn't good enough and keep looking for a better one\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#Overfitting and Underfitting\n",
    "\n",
    "'''\n",
    "A common danger in ML is overfitting - producing a model that performs well on the data you train it on but generalizes poorly to any new data\n",
    "The other side of this is underfitting - producing a model that doesn't perform well even on the training data, although typically when this happens you decide your model isn't good enough and keep looking for a better one\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using different data to train the model and to test the model. Simplest way to do this is to split the dataset, so that 2/3 of it is used to train the model, after which we measure the model's performance on the remaining 1/3\n",
    "\n",
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "X = TypeVar('X') #generic type to represent a data point\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                  #Make a shallow copy\n",
    "    random.shuffle(data)            #b/c shuffle modifies the list\n",
    "    cut = int(len(data) * prob)     #Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]   #and split the shuffled list there\n",
    "\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.75)\n",
    "\n",
    "#The proportions should be correct\n",
    "assert len(train) == 750\n",
    "assert len(test) == 250\n",
    "\n",
    "#And the original data should be preserved (in some order)\n",
    "assert sorted(train + test) == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nAfter which you can do something like:\\n\\nmodel = SomeKindOfModel()\\nx_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.33)\\nmodel.train(x_train, y_train)\\nperformance = model.test(x_test, y_test)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#Often, we'll have paired input variables and output variables. In that case, we need to make sure to put corresponding values together in either the training data or the test data\n",
    "\n",
    "Y = TypeVar('Y') #generic type to represent output variables\n",
    "\n",
    "def train_test_split(xs: List[X], ys: List[Y], test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    # Generate the indices and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],    #x_train\n",
    "            [xs[i] for i in test_idxs],     #x_test\n",
    "            [ys[i] for i in train_idxs],    #y_train\n",
    "            [ys[i] for i in test_idxs])     #y_test\n",
    "\n",
    "xs = [x for x in range(1000)] # xs are 1 ... 10000\n",
    "ys = [2* x for x in xs]       #each y_i is twice x_i\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "\n",
    "#Check that the proportions are correct\n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(x_test) == len(y_test) == 250\n",
    "\n",
    "#Check that the corresponding data points are paired correctly\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))\n",
    "assert all(y == 2 * x for x, y in zip(x_test, y_test))\n",
    "\n",
    "'''\n",
    "After which you can do something like:\n",
    "\n",
    "model = SomeKindOfModel()\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.33)\n",
    "model.train(x_train, y_train)\n",
    "performance = model.test(x_test, y_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nGiven a set of labeled data and such a predictive model, every data point lies in one of four categories\\n    True Positive\\n        \"This message is spam, and we correctly prodicted spam\"\\n    False Positive (Type 1 error)\\n        \"This message is not spam, but we predicted spam\"\\n    False Negative (Type 2 error)\\n        \"This message is spam, but we predicted no spam\"\\n    True Negative\\n        \"This message is not spam, and we correctly predicted not spam\"\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#Correctness\n",
    "\n",
    "'''\n",
    "Given a set of labeled data and such a predictive model, every data point lies in one of four categories\n",
    "    True Positive\n",
    "        \"This message is spam, and we correctly prodicted spam\"\n",
    "    False Positive (Type 1 error)\n",
    "        \"This message is not spam, but we predicted spam\"\n",
    "    False Negative (Type 2 error)\n",
    "        \"This message is spam, but we predicted no spam\"\n",
    "    True Negative\n",
    "        \"This message is not spam, and we correctly predicted not spam\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy - the fraction of correct prediction\n",
    "\n",
    "#tn = true Negative, fn = False negative, tp = True Positive, tn = True Negative\n",
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct = tp +tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total\n",
    "\n",
    "assert accuracy(70, 4930, 13930, 981070) == 0.98114\n",
    "\n",
    "#Precision measures how accurate our positive predictions were\n",
    "\n",
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "assert precision(70, 4930, 13930, 981070) == 0.014\n",
    "\n",
    "#Recall measures what fraction of the positives our model identified\n",
    "\n",
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "assert recall(70, 4930, 13930, 981070) == 0.005\n",
    "\n",
    "#Sometimes precision and recall are combined into the F1 score\n",
    "\n",
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "\n",
    "    return 2 * p * r / (p+r)\n",
    "\n",
    "#This is the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    Another way of thinking about the overfitting problem is as a tradeoff between bias and variance. Both are measures of what would happen if you were to retrain your model many times on different sets of training data(from the same larger population)\\n    If your model has high bias (which means it performs poorly even on your training data), one thing to try is adding more features.\\n    If your model has high variance, you can similarly remove features. But another solution is to obtain more data (if you can)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#The Bias-Variance Tradeoff\n",
    "'''\n",
    "    Another way of thinking about the overfitting problem is as a tradeoff between bias and variance. Both are measures of what would happen if you were to retrain your model many times on different sets of training data(from the same larger population)\n",
    "    If your model has high bias (which means it performs poorly even on your training data), one thing to try is adding more features.\n",
    "    If your model has high variance, you can similarly remove features. But another solution is to obtain more data (if you can)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Selection\n",
    "'''\n",
    "    Features are whatever inputs we provide to our model\n",
    "\n",
    "'''"
   ]
  }
 ]
}