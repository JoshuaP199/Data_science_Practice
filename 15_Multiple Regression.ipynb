{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 49, 4, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#The Model\n",
    "'''\n",
    "    You've collected additional data: you know how many hours each of your users works each day, and whether they have a PhD. You'd like to use this additional data to improve your model.\n",
    "    Accordingly, you hypothesize a linear model with more iundependent variables:\n",
    "\n",
    "        minutes = α + β1friends + β2work hours + β3phd + ε\n",
    "    \n",
    "    Obviously, whether a user has a PhD is not a number - but, as we mentioned in Chp 11, we can introduce a dummy variable that equals 1 for users with PhDs and 0 for users without, after it's just as numberic as the other variables.\n",
    "\n",
    "    Recall that in Chp 14 we git a model of the form:\n",
    "        \n",
    "        yi = α + βxi + εi\n",
    "    \n",
    "    Now imagine that each inputn xi is not a single number but rather a vector of k numbers, xi1, ..., xik. The multiple regression model assumes that:\n",
    "\n",
    "        yi = α + β1xi1 + ... + βkxik + εi\n",
    "    \n",
    "    In mutliple regression the vector of parameters is usually called β. We'll want this to include the constant term as well, which we can achieve by adding a column of 1s to our data:\n",
    "\n",
    "        beta = [alpha, beta_1, ..., beta_k]\n",
    "\n",
    "    and:\n",
    "\n",
    "        x_i = 1, x_i1, ..., x_ik\n",
    "\n",
    "    Then our model is just:\n",
    "'''\n",
    "\n",
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "#In this particular case, our independent variable x will be a list of vectors, each of which looks like this:\n",
    "\n",
    "[1,     #constant term\n",
    " 49,    #number of friends\n",
    " 4,     #work hours per day\n",
    " 0]     #doesn't have PhD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    There are a couple of further assumptions that are required for this model (and our solution) to make sense.\\n    The first is that the columns of x are linearly independent - that there\\'s no way to write any one as a weighted sum of some of the others. If this assumption fails, it\\'s impossible to estimate beta. To see this in an extreme case, imagine we had an extra field num_acquanintances in our data that for every user was exactly equal to num_friends.\\n    Then, starting with any beta, if we add any amount to the num_friends coefficient and subtract that same amount form the num_acquaintances coefficient, the model\\'s predictions will remain unchanged. This means that there\\'s no way to find the coefficient for num_friends.(Usually violations of this assumption won\\'t be obvious)\\n    The second important assumption is that the columns of x are all uncorrelated with the errors ε. If this fails to be the case, our estimates of beta will be systematically wrong.\\n    For instance, in Chp. 14, we build a model thatpredicted that each additional friend was associated with an extra 0.90 daily minutes on the site.\\n    Imagine it\\'s also the case that:\\n        \\n        -People who work more hours spend less time on the site\\n        -People with mre friends tend to work more hours\\n\\n    That is, imagine that the \"actual\" model is:\\n\\n        minutes = α + β1friends + β2work hours + ε\\n\\n    Where β2 is negative, and that work hours and friends are positively correlated. In that case, when we minimize the errors of the single-variable model:\\n    \\n        minutes = α + β1friends + ε\\n\\n    we will underestimate β1.\\n\\n    Think about what would happen if we made predictions using the single-variable model with the \"actual\" value of β1. (That is, the value that arises from minimizing the errors of what we called the \"actual\" model.) The predictions would tend to be way too large for users who work many hours and a little too large for users who work few hours, because β2 < 0 and we \"forgot\" to include it. Because work hours is positively correlated with number of friends, this means the predictions tend to be way too large for users with many friends, and the only slightly too large for users with few friends.\\n    The result of this is that we canr educe the errors (in the single-variable model) by decreasing our estimate of β1, which means that the error-minimizing β1 is smaller than the \"actual\" value. That is, in this case the single-variable least squares solution is biased to underestimating β1. And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of β1.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#Further Assumptions of the Least Squares Model\n",
    "'''\n",
    "    There are a couple of further assumptions that are required for this model (and our solution) to make sense.\n",
    "    The first is that the columns of x are linearly independent - that there's no way to write any one as a weighted sum of some of the others. If this assumption fails, it's impossible to estimate beta. To see this in an extreme case, imagine we had an extra field num_acquanintances in our data that for every user was exactly equal to num_friends.\n",
    "    Then, starting with any beta, if we add any amount to the num_friends coefficient and subtract that same amount form the num_acquaintances coefficient, the model's predictions will remain unchanged. This means that there's no way to find the coefficient for num_friends.(Usually violations of this assumption won't be obvious)\n",
    "    The second important assumption is that the columns of x are all uncorrelated with the errors ε. If this fails to be the case, our estimates of beta will be systematically wrong.\n",
    "    For instance, in Chp. 14, we build a model thatpredicted that each additional friend was associated with an extra 0.90 daily minutes on the site.\n",
    "    Imagine it's also the case that:\n",
    "        \n",
    "        -People who work more hours spend less time on the site\n",
    "        -People with mre friends tend to work more hours\n",
    "\n",
    "    That is, imagine that the \"actual\" model is:\n",
    "\n",
    "        minutes = α + β1friends + β2work hours + ε\n",
    "\n",
    "    Where β2 is negative, and that work hours and friends are positively correlated. In that case, when we minimize the errors of the single-variable model:\n",
    "    \n",
    "        minutes = α + β1friends + ε\n",
    "\n",
    "    we will underestimate β1.\n",
    "\n",
    "    Think about what would happen if we made predictions using the single-variable model with the \"actual\" value of β1. (That is, the value that arises from minimizing the errors of what we called the \"actual\" model.) The predictions would tend to be way too large for users who work many hours and a little too large for users who work few hours, because β2 < 0 and we \"forgot\" to include it. Because work hours is positively correlated with number of friends, this means the predictions tend to be way too large for users with many friends, and the only slightly too large for users with few friends.\n",
    "    The result of this is that we canr educe the errors (in the single-variable model) by decreasing our estimate of β1, which means that the error-minimizing β1 is smaller than the \"actual\" value. That is, in this case the single-variable least squares solution is biased to underestimating β1. And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of β1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the Model\n",
    "'''\n",
    "    As we did in the simple linear model, we'll choose beta to minimize the sum of squared errors. Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent. Again we'll want to minimize the sum of the squared errors. The error function is almost identical to the one used in Chp14, except that instead of expecting parameters [alpha, beta] it will take a vector of arbitrary length:\n",
    "'''\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x,y,beta) ** 2\n",
    "\n",
    "x = [1,2,3]\n",
    "y= 30\n",
    "beta = [4,4,4]  #so prediction = 4 + 8 + 12 = 24\n",
    "                #             (1*4) + (2*4) + (3*4) = 24\n",
    "\n",
    "assert error(x,y,beta) == -6            #24-30 \n",
    "assert squared_error(x,y,beta) == 36    #-6**2\n",
    "\n",
    "#If you know calculus, it's easy to compute the gradient\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x,y,beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x,y,beta) == [-12, -24, -36]\n",
    "#err = -6 -> [[2 * -6 * 1], [2 * -6 * 2], [2 * -6 * 3]] -> [-12, -24, -36]\n",
    "\n",
    "'''\n",
    "    Otherwise, you'll need to take my word for it. \n",
    "    At this point, we're ready to find the optimal beta using gradient descent. Let's first write out a least_squares_fit function that can work with any dataset:\n",
    "'''\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    '''\n",
    "    Find the beta that minimizes the sum of squared errors \n",
    "    assuming the model y = dot(x, beta)\n",
    "    '''\n",
    "\n",
    "    #start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "        \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "least squares fit: 100%|██████████| 5000/5000 [00:01<00:00, 4222.02it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n    In practice, you wouldn't estimate a linear regression using gradient descent; you'd get the exact coefficient using linear algebra techniques that are beyond the scope of this book. If you did so, you'd find the equation:\\n\\n        mintutes = 30.58 + 0.972 friends - 1.87 work hours + 0.923 phd\\n\\n    which is pretty close to what we found\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "#We can then apply that to our data\n",
    "\n",
    "from scratch.statistics import daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "random.seed(0)\n",
    "learning_rate = 0.001\n",
    "\n",
    "#No idea what 'inputs' is exactly, I understand it's the data about the users but was never given exactly what it was, to get these assert's to pass besides the random.seed\n",
    "beta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25)\n",
    "'''assert 30.50 < beta[0] < 30.70  #constant\n",
    "assert  0.96 < beta[1] <  1.00  #num friends\n",
    "assert -1.89 < beta[2] < -1.85  #work hours per day\n",
    "assert  0.91 < beta[3] <  0.94  #has PhD'''\n",
    "\n",
    "\n",
    "'''\n",
    "    In practice, you wouldn't estimate a linear regression using gradient descent; you'd get the exact coefficient using linear algebra techniques that are beyond the scope of this book. If you did so, you'd find the equation:\n",
    "\n",
    "        mintutes = 30.58 + 0.972 friends - 1.87 work hours + 0.923 phd\n",
    "\n",
    "    which is pretty close to what we found\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    You should think of the coefficients of the model as representing all-else-being-equal estimates of the impacts of each factor. All else being equal, each additional friend corresponds to an extra minute spent on the site each day. All else being equal, each additional hour in a user\\'s workday corresponds to about two fewer minutes spent on the site each day. All else being equal, having a PhD is associated with spending an extra minute on the site each day.\\n    What this doesn\\'t (directly) tell us is anything about the interactions among the variables. It\\'s possible that the effect of work hours is different for people with many friends than it is for people with few friends. This model doesn\\'t capture that. One way to handle this case is to introduce a new variable that is the product of \"friends\" and \"work hours.\" This effectively allows the \"work hours\" coefficient to increase (or decrease) as the number of friends increases.\\n    Or it\\'s possible that the more friends you have, the more time you spend on the site yp to a point, after which further friends cause you to spend less time on the site. (Perhaps with too many friends the experience is just too overwhelming?) We could try to capture this in our model by adding another variable that\\'s the square of the number of friends.\\n    Once we start adding variables, we need to worry about whether their coefficients \"matter.\" There are no limits to the numbers of products, logs, squares, and higher powers we could add.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#Interpreting the Model\n",
    "'''\n",
    "    You should think of the coefficients of the model as representing all-else-being-equal estimates of the impacts of each factor. All else being equal, each additional friend corresponds to an extra minute spent on the site each day. All else being equal, each additional hour in a user's workday corresponds to about two fewer minutes spent on the site each day. All else being equal, having a PhD is associated with spending an extra minute on the site each day.\n",
    "    What this doesn't (directly) tell us is anything about the interactions among the variables. It's possible that the effect of work hours is different for people with many friends than it is for people with few friends. This model doesn't capture that. One way to handle this case is to introduce a new variable that is the product of \"friends\" and \"work hours.\" This effectively allows the \"work hours\" coefficient to increase (or decrease) as the number of friends increases.\n",
    "    Or it's possible that the more friends you have, the more time you spend on the site yp to a point, after which further friends cause you to spend less time on the site. (Perhaps with too many friends the experience is just too overwhelming?) We could try to capture this in our model by adding another variable that's the square of the number of friends.\n",
    "    Once we start adding variables, we need to worry about whether their coefficients \"matter.\" There are no limits to the numbers of products, logs, squares, and higher powers we could add.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'function'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b5cdec202111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Which has now increased to 0.68:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;36m0.67\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmultiple_r_squared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.68\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-b5cdec202111>\u001b[0m in \u001b[0;36mmultiple_r_squared\u001b[1;34m(xs, ys, beta)\u001b[0m\n\u001b[0;32m      7\u001b[0m     sum_of_squared_errors = sum(error(x,y,beta) ** 2\n\u001b[0;32m      8\u001b[0m                                 for x, y in zip(xs, ys))\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msum_of_squared_errors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_sum_of_squares\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Which has now increased to 0.68:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'function'"
     ]
    }
   ],
   "source": [
    "#Goodness of Fit\n",
    "\n",
    "#Again we can look at R-squared:\n",
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x,y,beta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares\n",
    "\n",
    "#Which has now increased to 0.68:\n",
    "#assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68\n",
    "\n",
    "'''\n",
    "    Keep in mind, however, that adding new variables to a regression will necessarily increase the R-squared.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}