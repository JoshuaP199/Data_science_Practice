{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 49, 4, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#The Model\n",
    "'''\n",
    "    You've collected additional data: you know how many hours each of your users works each day, and whether they have a PhD. You'd like to use this additional data to improve your model.\n",
    "    Accordingly, you hypothesize a linear model with more iundependent variables:\n",
    "\n",
    "        minutes = α + β1friends + β2work hours + β3phd + ε\n",
    "    \n",
    "    Obviously, whether a user has a PhD is not a number - but, as we mentioned in Chp 11, we can introduce a dummy variable that equals 1 for users with PhDs and 0 for users without, after it's just as numberic as the other variables.\n",
    "\n",
    "    Recall that in Chp 14 we git a model of the form:\n",
    "        \n",
    "        yi = α + βxi + εi\n",
    "    \n",
    "    Now imagine that each inputn xi is not a single number but rather a vector of k numbers, xi1, ..., xik. The multiple regression model assumes that:\n",
    "\n",
    "        yi = α + β1xi1 + ... + βkxik + εi\n",
    "    \n",
    "    In mutliple regression the vector of parameters is usually called β. We'll want this to include the constant term as well, which we can achieve by adding a column of 1s to our data:\n",
    "\n",
    "        beta = [alpha, beta_1, ..., beta_k]\n",
    "\n",
    "    and:\n",
    "\n",
    "        x_i = 1, x_i1, ..., x_ik\n",
    "\n",
    "    Then our model is just:\n",
    "'''\n",
    "\n",
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "#In this particular case, our independent variable x will be a list of vectors, each of which looks like this:\n",
    "\n",
    "[1,     #constant term\n",
    " 49,    #number of friends\n",
    " 4,     #work hours per day\n",
    " 0]     #doesn't have PhD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    There are a couple of further assumptions that are required for this model (and our solution) to make sense.\\n    The first is that the columns of x are linearly independent - that there\\'s no way to write any one as a weighted sum of some of the others. If this assumption fails, it\\'s impossible to estimate beta. To see this in an extreme case, imagine we had an extra field num_acquanintances in our data that for every user was exactly equal to num_friends.\\n    Then, starting with any beta, if we add any amount to the num_friends coefficient and subtract that same amount form the num_acquaintances coefficient, the model\\'s predictions will remain unchanged. This means that there\\'s no way to find the coefficient for num_friends.(Usually violations of this assumption won\\'t be obvious)\\n    The second important assumption is that the columns of x are all uncorrelated with the errors ε. If this fails to be the case, our estimates of beta will be systematically wrong.\\n    For instance, in Chp. 14, we build a model thatpredicted that each additional friend was associated with an extra 0.90 daily minutes on the site.\\n    Imagine it\\'s also the case that:\\n        \\n        -People who work more hours spend less time on the site\\n        -People with mre friends tend to work more hours\\n\\n    That is, imagine that the \"actual\" model is:\\n\\n        minutes = α + β1friends + β2work hours + ε\\n\\n    Where β2 is negative, and that work hours and friends are positively correlated. In that case, when we minimize the errors of the single-variable model:\\n    \\n        minutes = α + β1friends + ε\\n\\n    we will underestimate β1.\\n\\n    Think about what would happen if we made predictions using the single-variable model with the \"actual\" value of β1. (That is, the value that arises from minimizing the errors of what we called the \"actual\" model.) The predictions would tend to be way too large for users who work many hours and a little too large for users who work few hours, because β2 < 0 and we \"forgot\" to include it. Because work hours is positively correlated with number of friends, this means the predictions tend to be way too large for users with many friends, and the only slightly too large for users with few friends.\\n    The result of this is that we canr educe the errors (in the single-variable model) by decreasing our estimate of β1, which means that the error-minimizing β1 is smaller than the \"actual\" value. That is, in this case the single-variable least squares solution is biased to underestimating β1. And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of β1.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#Further Assumptions of the Least Squares Model\n",
    "'''\n",
    "    There are a couple of further assumptions that are required for this model (and our solution) to make sense.\n",
    "    The first is that the columns of x are linearly independent - that there's no way to write any one as a weighted sum of some of the others. If this assumption fails, it's impossible to estimate beta. To see this in an extreme case, imagine we had an extra field num_acquanintances in our data that for every user was exactly equal to num_friends.\n",
    "    Then, starting with any beta, if we add any amount to the num_friends coefficient and subtract that same amount form the num_acquaintances coefficient, the model's predictions will remain unchanged. This means that there's no way to find the coefficient for num_friends.(Usually violations of this assumption won't be obvious)\n",
    "    The second important assumption is that the columns of x are all uncorrelated with the errors ε. If this fails to be the case, our estimates of beta will be systematically wrong.\n",
    "    For instance, in Chp. 14, we build a model thatpredicted that each additional friend was associated with an extra 0.90 daily minutes on the site.\n",
    "    Imagine it's also the case that:\n",
    "        \n",
    "        -People who work more hours spend less time on the site\n",
    "        -People with mre friends tend to work more hours\n",
    "\n",
    "    That is, imagine that the \"actual\" model is:\n",
    "\n",
    "        minutes = α + β1friends + β2work hours + ε\n",
    "\n",
    "    Where β2 is negative, and that work hours and friends are positively correlated. In that case, when we minimize the errors of the single-variable model:\n",
    "    \n",
    "        minutes = α + β1friends + ε\n",
    "\n",
    "    we will underestimate β1.\n",
    "\n",
    "    Think about what would happen if we made predictions using the single-variable model with the \"actual\" value of β1. (That is, the value that arises from minimizing the errors of what we called the \"actual\" model.) The predictions would tend to be way too large for users who work many hours and a little too large for users who work few hours, because β2 < 0 and we \"forgot\" to include it. Because work hours is positively correlated with number of friends, this means the predictions tend to be way too large for users with many friends, and the only slightly too large for users with few friends.\n",
    "    The result of this is that we canr educe the errors (in the single-variable model) by decreasing our estimate of β1, which means that the error-minimizing β1 is smaller than the \"actual\" value. That is, in this case the single-variable least squares solution is biased to underestimating β1. And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of β1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the Model\n",
    "'''\n",
    "    As we did in the simple linear model, we'll choose beta to minimize the sum of squared errors. Finding an exact solution is not simple to do by hand, which means we'll need to use gradient descent. Again we'll want to minimize the sum of the squared errors. The error function is almost identical to the one used in Chp14, except that instead of expecting parameters [alpha, beta] it will take a vector of arbitrary length:\n",
    "'''\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x,y,beta) ** 2\n",
    "\n",
    "x = [1,2,3]\n",
    "y= 30\n",
    "beta = [4,4,4]  #so prediction = 4 + 8 + 12 = 24\n",
    "                #             (1*4) + (2*4) + (3*4) = 24\n",
    "\n",
    "assert error(x,y,beta) == -6            #24-30 \n",
    "assert squared_error(x,y,beta) == 36    #-6**2\n",
    "\n",
    "#If you know calculus, it's easy to compute the gradient\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x,y,beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x,y,beta) == [-12, -24, -36]\n",
    "#err = -6 -> [[2 * -6 * 1], [2 * -6 * 2], [2 * -6 * 3]] -> [-12, -24, -36]\n",
    "\n",
    "'''\n",
    "    Otherwise, you'll need to take my word for it. \n",
    "    At this point, we're ready to find the optimal beta using gradient descent. Let's first write out a least_squares_fit function that can work with any dataset:\n",
    "'''\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    '''\n",
    "    Find the beta that minimizes the sum of squared errors \n",
    "    assuming the model y = dot(x, beta)\n",
    "    '''\n",
    "\n",
    "    #start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "        \n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-da90af3d6c5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#No idea what 'inputs' is exactly, I understand it's the data about the users but was never given exactly what it was, to get these assert's to pass besides the random.seed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m '''assert 30.50 < beta[0] < 30.70  #constant\n\u001b[0;32m     12\u001b[0m \u001b[1;32massert\u001b[0m  \u001b[1;36m0.96\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m  \u001b[1;36m1.00\u001b[0m  \u001b[1;31m#num friends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "#We can then apply that to our data\n",
    "\n",
    "from scratch.statistics import daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "random.seed(0)\n",
    "learning_rate = 0.001\n",
    "\n",
    "#No idea what 'inputs' is exactly, I understand it's the data about the users but was never given exactly what it was, to get these assert's to pass besides the random.seed\n",
    "beta = least_squares_fit(inputs, daily_minutes_good, learning_rate, 5000, 25)\n",
    "'''assert 30.50 < beta[0] < 30.70  #constant\n",
    "assert  0.96 < beta[1] <  1.00  #num friends\n",
    "assert -1.89 < beta[2] < -1.85  #work hours per day\n",
    "assert  0.91 < beta[3] <  0.94  #has PhD'''\n",
    "\n",
    "\n",
    "'''\n",
    "    In practice, you wouldn't estimate a linear regression using gradient descent; you'd get the exact coefficient using linear algebra techniques that are beyond the scope of this book. If you did so, you'd find the equation:\n",
    "\n",
    "        mintutes = 30.58 + 0.972 friends - 1.87 work hours + 0.923 phd\n",
    "\n",
    "    which is pretty close to what we found\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    You should think of the coefficients of the model as representing all-else-being-equal estimates of the impacts of each factor. All else being equal, each additional friend corresponds to an extra minute spent on the site each day. All else being equal, each additional hour in a user\\'s workday corresponds to about two fewer minutes spent on the site each day. All else being equal, having a PhD is associated with spending an extra minute on the site each day.\\n    What this doesn\\'t (directly) tell us is anything about the interactions among the variables. It\\'s possible that the effect of work hours is different for people with many friends than it is for people with few friends. This model doesn\\'t capture that. One way to handle this case is to introduce a new variable that is the product of \"friends\" and \"work hours.\" This effectively allows the \"work hours\" coefficient to increase (or decrease) as the number of friends increases.\\n    Or it\\'s possible that the more friends you have, the more time you spend on the site yp to a point, after which further friends cause you to spend less time on the site. (Perhaps with too many friends the experience is just too overwhelming?) We could try to capture this in our model by adding another variable that\\'s the square of the number of friends.\\n    Once we start adding variables, we need to worry about whether their coefficients \"matter.\" There are no limits to the numbers of products, logs, squares, and higher powers we could add.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#Interpreting the Model\n",
    "'''\n",
    "    You should think of the coefficients of the model as representing all-else-being-equal estimates of the impacts of each factor. All else being equal, each additional friend corresponds to an extra minute spent on the site each day. All else being equal, each additional hour in a user's workday corresponds to about two fewer minutes spent on the site each day. All else being equal, having a PhD is associated with spending an extra minute on the site each day.\n",
    "    What this doesn't (directly) tell us is anything about the interactions among the variables. It's possible that the effect of work hours is different for people with many friends than it is for people with few friends. This model doesn't capture that. One way to handle this case is to introduce a new variable that is the product of \"friends\" and \"work hours.\" This effectively allows the \"work hours\" coefficient to increase (or decrease) as the number of friends increases.\n",
    "    Or it's possible that the more friends you have, the more time you spend on the site yp to a point, after which further friends cause you to spend less time on the site. (Perhaps with too many friends the experience is just too overwhelming?) We could try to capture this in our model by adding another variable that's the square of the number of friends.\n",
    "    Once we start adding variables, we need to worry about whether their coefficients \"matter.\" There are no limits to the numbers of products, logs, squares, and higher powers we could add.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    Keep in mind, however, that adding new variables to a regression will necessarily increase the R-squared.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "#Goodness of Fit\n",
    "\n",
    "#Again we can look at R-squared:\n",
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: Vector, beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum(error(x,y,beta) ** 2\n",
    "                                for x, y in zip(xs, ys))\n",
    "    return 1.0 - sum_of_squared_errors / total_sum_of_squares\n",
    "\n",
    "#Which has now increased to 0.68:\n",
    "#assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68\n",
    "\n",
    "'''\n",
    "    Keep in mind, however, that adding new variables to a regression will necessarily increase the R-squared. After all, the simple regression model is just the special case of the multiple regression model where the coefficients on \"work hours\" and \"PhD\" both equal 0. The optimal multiple regression model will necessarilyhave an error at least as small as that one.\n",
    "    Because of this, in a multiple regression, we also need to look at the standard errors of the coefficients, which measures how certain we are about our estimates of each βi. The regression as a whole may fit our data wellm but if some of the independent variables are correlated (or irrelevant), their coefficients might not mean much. \n",
    "    The typical approach to measuring these errors starts with another assumption - that the errors εi are independent normal random variables with mean 0 and some shared (unknown) standard deviation σ. In that case, we (or, more likely, our statistical software) can use some linear algebra to find the standard error of each coefficient. The larger it is, the less sure our model is about that coefficient. Unfortunately, we're not set up to do that kind of linear algebra from scratch.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Digression: The Bootstrap\n",
    "'''\n",
    "    Imagine that we have a sample of n data points, generated by some (unknown to us) distrubution:\n",
    "\n",
    "data = get_sample(num_points = n)\n",
    "\n",
    "    In Chp. 5, we wrote a function that could compute the median of the sample, which we can use as an estimate of the median of the distribution itself.\n",
    "    But how confident can we be about our estimate? If all the data points in the sample are very close to 100, then it seems likely that the actual median is close to 100. If approximately half the data points in the sample are c;pse to 0 and the other half are close to 200, then we can't be nearly as certain about the median.\n",
    "    If we could repeatedly get new samples, we could compute the medians of many samples and look at the distribution of those medians. Often we can't. In that case we can bootstrap new datasets by choosing n data points with repleacement from our data. And then we can compute the medians of those synthetic datasets:\n",
    "'''\n",
    "\n",
    "from typing import TypeVar, Callable\n",
    "\n",
    "X = TypeVar('X')        #Generic type for data\n",
    "Stat = TypeVar(\"Stat\")  #Generic type for \"statistic\"\n",
    "\n",
    "def bootstrap_sample(data: List[X]) -> List[X]:\n",
    "    \"\"\"randomly samples len(data) elements with replacement\"\"\"\n",
    "    return [random.choice(data) for  _ in data]\n",
    "\n",
    "def bootstrap_statistics(data: List[X], stats_fn: Callable[[List[X]], Stat], num_samples: int) -> List[Stat]:\n",
    "    \"\"\"evaluates stats_fn on num_samples bootstrap samples from data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_samples)]\n",
    "\n",
    "#For example, consider the two following datasets:\n",
    "\n",
    "#101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "#101 points, 50 of them are near 0, 50 of them are near 200\n",
    "far_from_100 = ([99.5 + random.random()] + [random.random() for _ in range(50)] + [200 + random.random() for _ in range(50)])\n",
    "\n",
    "'''\n",
    "    If you compute the medians of the two datasets, both will be very close to 100. However, if you look at:\n",
    "'''\n",
    "\n",
    "from scratch.statistics import median, standard_deviation\n",
    "\n",
    "medians_close = bootstrap_statistics(close_to_100, median, 100)\n",
    "\n",
    "''' You will mostly seenumbers really lcose to 100. But if you look at: '''\n",
    "\n",
    "medians_far = bootstrap_statistics(far_from_100, median, 100)\n",
    "\n",
    "'''\n",
    "    You will see a lot of numbers close to 0 and a lot of numbers close to 200.\n",
    "    The standard_deviation of the firts set of medians is close to 0, while that of the second set of medians is close to 100:\n",
    "'''\n",
    "\n",
    "assert standard_deviation(medians_close) < 1\n",
    "assert standard_deviation(medians_far) > 90\n",
    "\n",
    "#(This extreme a case would be pretty easy to figure out by manually insprecting the data, but in general that won't be true.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7465cfd739d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#This will take a couple of minutes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mbootstrap_betas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbootstrap_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimate_sample_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m '''\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "#Standard Errors of Regression Coefficients\n",
    "\n",
    "'''\n",
    "    We can take the same approach to estimating the standard errprs of our regrssion coefficients. We repeatedly take a bootstrap_sample of our data and estimate beta based on that sample. If the coefficient corresponding to one of the independent variables (say, num_friends) doesn't vary much across samples, then we can be confident that our estimate is relatively tight. If the coefficient varies greatly across the samples, then we can't be at all in our estimate.\n",
    "    The only subtlety is that, before sampling, we'll need to zip our x data and y data to make sure that corresponding values of the independent and dependent variables are sampled together. This means that bootstrap_sample will return a list of pairs (x_i, y_i),which we'll need to reassemble into an x_sample and a y_sample:\n",
    "'''\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import datetime\n",
    "\n",
    "def estimate_sample_beta(pairs: List[Tuple[Vector, float]]):\n",
    "    x_sample = [x for x, _ in pairs]\n",
    "    y_sample = [y for _, y in pairs]\n",
    "    beta = least_squares_fit(x_sample, y_sample, learning_rate, 5000, 25)\n",
    "    print(\"bootstrap sample\", beta)\n",
    "    return beta\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "#This will take a couple of minutes\n",
    "bootstrap_betas = bootstrap_statistics(list(zip(inputs, daily_minutes_good)), estimate_sample_beta, 100)\n",
    "\n",
    "'''\n",
    "    After which we can estimate the standard deviation of each coefficient:\n",
    "'''\n",
    "\n",
    "bootstrap_standard_errors = [standard_deviation([beta[i] for beta in bootstrap_betas]) for i in range(4)]\n",
    "\n",
    "print(bootstrap_standard_errors)\n",
    "\n",
    "'''\n",
    "    (We chould likely get better estimates if we collected more than 100 samples and used more than 5,000 iterations to estimate each beta, but we don't have all day.)\n",
    "    We can use these to test hypotheses such as \"does βi equal 0?\" Under the null hypothesis βi = 0 (and with our other assumptions about the distribution of εi), the statistics:\n",
    "\n",
    "    tj = β(hat)j / σ(hat)j\n",
    "\n",
    "    Which is our eatimate of βj divided by our estimate of its standard error, follows a Students t-distribution with \"n-k degrees of freedom.\"\n",
    "    If we had a students_t_cdf function, we could compute p-values for each least-squares coefficient to indicate how likely we would be to observe such a value if the actual coefficient were 0. Unfortunately, we don't have such a function. (Although we would if we weren't working from scratch.)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    (In a situation not like this, we would probably be using statistical software that knows how to compute the t-distribution, as well as how to compute the exact standard errors.)\\n    While most of the coefficients have very small p-values (suggesting that they are indeed nonzero), the coefficient for \"PhD\" is not \"Significantly\" different from 0, which makes it likely that the coefficient for \"PhD\" is random rather than meaningful\\n    In more elaborate regression scenarios, you sometimes want to test more elaborate hypotheses about the data, such as \"at least one of the βj is nonzero\" or \"β1 equals β2 and β3 equals β4.\" You can do this with an F-test, but alas, that falls outside the scope of this book.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "''' \n",
    "    However, as the degrees of freedom get large, the t-distribution gets closer and closer to a standard normal. In a situation like this, where n is much larger than k, we can use normal_cdf and still feel good about ourselves:\n",
    "'''\n",
    "\n",
    "from scratch.probability import normal_cdf\n",
    "\n",
    "def p_value(beta_hat_j: float, sigma_hat_j: float) -> float:\n",
    "    if beta_hat_j > 0:\n",
    "        #If the coefficient is positive, we need to compute twice the probability of seeing an eve *larger* value\n",
    "        return 2 * (1- normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "    else:\n",
    "        #otherwise twice the probaility of seeing a *smaller* value\n",
    "        return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "\n",
    "assert p_value(30.58, 1.27) < 0.001     #constant term\n",
    "assert p_value(0.972, 0.103) < 0.001    #num_friends\n",
    "assert p_value(-1.865, 0.155) < 0.001   #work_hours\n",
    "assert p_value(0.923, 1.249) > 0.4      #phd\n",
    "\n",
    "'''\n",
    "    (In a situation not like this, we would probably be using statistical software that knows how to compute the t-distribution, as well as how to compute the exact standard errors.)\n",
    "    While most of the coefficients have very small p-values (suggesting that they are indeed nonzero), the coefficient for \"PhD\" is not \"Significantly\" different from 0, which makes it likely that the coefficient for \"PhD\" is random rather than meaningful\n",
    "    In more elaborate regression scenarios, you sometimes want to test more elaborate hypotheses about the data, such as \"at least one of the βj is nonzero\" or \"β1 equals β2 and β3 equals β4.\" You can do this with an F-test, but alas, that falls outside the scope of this book.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2c2ef3b144d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m#again, no idea what his exact \"inputs\" are so it doesn't run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mbeta_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_fit_ridge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_minutes_good\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#[30.52, 0.97, -1.85, 0.91]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "#Regularization\n",
    "\n",
    "'''\n",
    "    In practice, you'd often like to apply linear regression to datasets with large numbers of varibales. This creates a couple of extra wrinkles. First, the more variables you use, the more likely you are to overfit your model to the training set. And second, the more nonzero coefficients you have, the hardeer it is to make sense of them. If the goal is to explain some phenomenon, a sparse model with three factors might be more useful than a slightly better model with hundreds.\n",
    "    Regularization is an approach in which we add to the error term a penalty that gets larger as beta gets larger. We then minimize the combined error and penalty. The more importance we place on the penalty term, the more we discourage large coefficients.\n",
    "    For example, in ridge regression, we add a penalty proportional to the sum of ther squares of the beta_i (except that typically we don't penalize beta_0, the constant term):\n",
    "'''\n",
    "\n",
    "#alpha is a *hyperparameter* controlling how harsh the penalty is.\n",
    "#Sometimes it's called \"lambda\" but that already means something in Py\n",
    "\n",
    "def ridge_penalty(beta: Vector, alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared__error_ridge(x: Vector,\n",
    "                         y: float,\n",
    "                         beta: Vector,\n",
    "                         alpha: float) -> float:\n",
    "    \"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "    return error(x,y,beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n",
    "#We can then plug this into gradient descent in the usual way:\n",
    "\n",
    "from scratch.linear_algebra import add\n",
    "\n",
    "def redige_penalty_gradient(beta: Vector, alpha: float) -> Vector:\n",
    "    \"\"\"gradient of just the ridge penalty\"\"\"\n",
    "    return [0.] + [2 * alpha * beta__j for beta_j in beta[1:]]\n",
    "\n",
    "def sqerror_ridge_gradient(x: Vector,\n",
    "                           y:float,\n",
    "                           beta: Vector,\n",
    "                           alpha: float) -> Vector:\n",
    "    \"\"\" the gradient corresponding to the ith squared error term inlcuding the ridge penalty\"\"\"\n",
    "    return add(sqerror_gradient(x,y,beta), redige_penalty_gradient(beta, alpha))\n",
    "\n",
    "'''\n",
    "    And then we just need to modify the least_squares_fit function to use the sqerror_ridge_gradient instead of sqerror_gradient. (I'm not going to repeat the code here.)\n",
    "    With alpha set to 0, there's no penalty at all and we get the same results as before:\n",
    "'''\n",
    "def least_squares_fit_ridge(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "\n",
    "    #start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_ridge_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "        \n",
    "    return guess\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "#again, no idea what his exact \"inputs\" are so it doesn't run\n",
    "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0, learning_rate, 5000, 25)\n",
    "\n",
    "#[30.52, 0.97, -1.85, 0.91]\n",
    "\n",
    "#As we increase alpha, the goodness of fit gets worse, but the size of beta gets smaller:\n",
    "\n",
    "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1, # alpha\n",
    "learning_rate, 5000, 25)\n",
    "\n",
    "# [30.8, 0.95, -1.83, 0.54]\n",
    "\n",
    "beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1, # alpha\n",
    "learning_rate, 5000, 25)\n",
    "\n",
    "# [30.6, 0.90, -1.68, 0.10]\n",
    "\n",
    "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good,10, # alpha\n",
    "learning_rate, 5000, 25)\n",
    "# [28.3, 0.67, -0.90, -0.01]\n",
    "\n",
    "'''\n",
    "    In particular, the coefficient on \"PhD\" vanishes as we increase the penalty, which accords with our previous result that it wasn't significantly different from 0.\n",
    "    Another approach is lasso regression, which uses the penalty:\n",
    "'''\n",
    "\n",
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])\n",
    "\n",
    "'''\n",
    "    Whereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to force coefficients to be 0, which makes it good for learning sparse models. Unfortunately, it's not amenable to gradient descent, which means that we won't be able to solve it from scratch.\n",
    "'''"
   ]
  }
 ]
}