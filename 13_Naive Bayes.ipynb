{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam messages do, then the probability that any given bitcoin-containing email is spam is:        0.5/(0.5 + 0.01) = 98%\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#A Really Dumb Spam Filter\n",
    "''' Bayes's theorem'''\n",
    "\n",
    "'''\n",
    "    P(\"the message is spam\" S / \"The message contains the word bitcoin\" B) = [P(B|S)P(S)] / [P(B|S)P(S) + P(B|-S)P(-S)] \n",
    "    (-S = message is not spam)\n",
    "'''\n",
    "'''\n",
    "    For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam messages do, then the probability that any given bitcoin-containing email is spam is:        0.5/(0.5 + 0.01) = 98%\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A More Sophisticated Spam Filter\n",
    "'''\n",
    "    Vocabulary of many words w1, ..., wn\n",
    "    Xi for the event \"a message contains the word wi\".     '\n",
    "    Imagine we've come up with an estimate P(Xi|S) for the probability that a spam message gonatins the ith word, and a similar estimate P(Xi|-S) for the probability that a nonspam message contains the ith word.\n",
    "\n",
    "    The key to Naive Bayes is making the (big) assumption that the presence (or absences) of each word are independent of one another, conditional on a message being spam or not.\n",
    "    This assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message ontains the word rolex. In math terms, this means that:\n",
    "        \n",
    "        P(X1 = x1, ..., Xn = xn|S) = P(X1 = x1|S) * ... * P(Xn=xn|S)\n",
    "    \n",
    "    This is an extreme assumption. (There's a reason the technique has naive in its name.) Imagine that our vocabulary consists only of the words bitcoin and rolex, and that half of all spam messages are for \"earn bitcoin\" and that the other half are for \"authentic rolex.\" In this case, the Naive Bayes estimate that a spam message contains both bitcoin and rolex is:\n",
    "\n",
    "        P(X1 = 1, X2 = 1|S) = P(X1= 1|S)P(X2 = 1|S) = .5 * .5 = .25\n",
    "\n",
    "    since we've assumed away the knowledge that bitcoin and rolex actually never occure together. Dispite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\n",
    "'''"
   ]
  }
 ]
}