{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam messages do, then the probability that any given bitcoin-containing email is spam is:        0.5/(0.5 + 0.01) = 98%\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#A Really Dumb Spam Filter\n",
    "''' Bayes's theorem'''\n",
    "\n",
    "'''\n",
    "    P(\"the message is spam\" S / \"The message contains the word bitcoin\" B) = [P(B|S)P(S)] / [P(B|S)P(S) + P(B|-S)P(-S)] \n",
    "    (-S = message is not spam)\n",
    "'''\n",
    "'''\n",
    "    For example, if 50% of spam messages have the word bitcoin, but only 1% of nonspam messages do, then the probability that any given bitcoin-containing email is spam is:        0.5/(0.5 + 0.01) = 98%\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    Vocabulary of many words w1, ..., wn\\n    Xi for the event \"a message contains the word wi\".     \\'\\n    Imagine we\\'ve come up with an estimate P(Xi|S) for the probability that a spam message gonatins the ith word, and a similar estimate P(Xi|-S) for the probability that a nonspam message contains the ith word.\\n\\n    The key to Naive Bayes is making the (big) assumption that the presence (or absences) of each word are independent of one another, conditional on a message being spam or not.\\n    This assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message ontains the word rolex. In math terms, this means that:\\n        \\n        P(X1 = x1, ..., Xn = xn|S) = P(X1 = x1|S) * ... * P(Xn=xn|S)\\n    \\n    This is an extreme assumption. (There\\'s a reason the technique has naive in its name.) Imagine that our vocabulary consists only of the words bitcoin and rolex, and that half of all spam messages are for \"earn bitcoin\" and that the other half are for \"authentic rolex.\" In this case, the Naive Bayes estimate that a spam message contains both bitcoin and rolex is:\\n\\n        P(X1 = 1, X2 = 1|S) = P(X1= 1|S)P(X2 = 1|S) = .5 * .5 = .25\\n\\n    since we\\'ve assumed away the knowledge that bitcoin and rolex actually never occure together. Dispite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#A More Sophisticated Spam Filter\n",
    "'''\n",
    "    Vocabulary of many words w1, ..., wn\n",
    "    Xi for the event \"a message contains the word wi\".     '\n",
    "    Imagine we've come up with an estimate P(Xi|S) for the probability that a spam message gonatins the ith word, and a similar estimate P(Xi|-S) for the probability that a nonspam message contains the ith word.\n",
    "\n",
    "    The key to Naive Bayes is making the (big) assumption that the presence (or absences) of each word are independent of one another, conditional on a message being spam or not.\n",
    "    This assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message ontains the word rolex. In math terms, this means that:\n",
    "        \n",
    "        P(X1 = x1, ..., Xn = xn|S) = P(X1 = x1|S) * ... * P(Xn=xn|S)\n",
    "    \n",
    "    This is an extreme assumption. (There's a reason the technique has naive in its name.) Imagine that our vocabulary consists only of the words bitcoin and rolex, and that half of all spam messages are for \"earn bitcoin\" and that the other half are for \"authentic rolex.\" In this case, the Naive Bayes estimate that a spam message contains both bitcoin and rolex is:\n",
    "\n",
    "        P(X1 = 1, X2 = 1|S) = P(X1= 1|S)P(X2 = 1|S) = .5 * .5 = .25\n",
    "\n",
    "    since we've assumed away the knowledge that bitcoin and rolex actually never occure together. Dispite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation (STUDY THIS AND UNDERSTAND COMPLETELY)\n",
    "'''\n",
    "    Let's create a simple function to tokenize messages into distinct words. We'll first convert each message to lowercase, then use re.findall to extract \"words\" consisting of letters, numbers, and apostrophes. Finally, we'll use set to get just the distinct words\n",
    "'''\n",
    "\n",
    "from typing import Set\n",
    "import re, tqdm\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()                             #Convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)      #extract the words, and\n",
    "    return set(all_words)                           #remove duplicates\n",
    "\n",
    "assert tokenize(\"Data Science is science\") == {\"data\", 'science', 'is'}\n",
    "\n",
    "#We'll also define a type for our training data\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "'''\n",
    "    As our classifier needs to keep track of tokens, counts, and labels from the training data, we'll make it a class. Following convention, we refer to nonspam emails as ham emails\n",
    "    The constructor will take just one parameter, the pseudocount to use when computing probabilities. It also initializes an empty set of tokens, counters to track how often each token is seen in spam messages and ham messages, and counts of how many spam and ham messages it has trained on:\n",
    "'''\n",
    "\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k #smoothing factor\n",
    "        \n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "\n",
    "#    Next, we'll give it a method to train it on a bunch of messages. First, we increment the spam_messages and ham_messages counts. Then we tokenize each message text, and for each token we increment the token_spam_counts or token_ham_counts based on the message type:\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            #Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            #Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "\n",
    "#    Ultimately we'll want to predict P(spam | token). As we saw earlier,  to apply Bayes's theorem we need to know P(token | spam) and P(token | ham) for each token in the vocabulary. So we'll create a \"private\" helper function to compute those:\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | ham)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "\n",
    "#    Finally, we're ready to write our predict method. As mentioned earlier, rather than multiplying together lots of small probabilities, we'll instead sum up the log probabilities:\n",
    "\n",
    "    def predict(self, text:str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        #Iterate through each word in our vocabulary\n",
    "        #attempt at using tqdm\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            #If *token* appears in the message,\n",
    "            #add the log probability of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            #Otherwise add the log probability of _not_ seeing it,\n",
    "            #which is log(1-probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "            \n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "#And now we have a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our Model\n",
    "\n",
    "messages = [Message(\"spam rules\", is_spam= True),\n",
    "            Message(\"ham rules\", is_spam= False),\n",
    "            Message(\"hello ham\", is_spam= False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k = 0.5)\n",
    "model.train(messages)\n",
    "\n",
    "#First, let's check that it got the counts right\n",
    "\n",
    "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n",
    "\n",
    "#Now let's make a prediction. We'll also go through our Naive Bayes logic by hand, and make sure that we get the same result\n",
    "\n",
    "text = \"hello spam\"\n",
    "\n",
    "probs_if_spam = [\n",
    "    (1 + 0.5) / (1 + 2 *0.5),           #\"spam\"  (present)\n",
    "    1 - (0 + 0.5) / (1 + 2 * 0.5),      #\"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (1 + 2 * 0.5),      #\"rules\" (not present)\n",
    "    (0 + 0.5) / (1 + 2 * 0.5)           #\"hello\" (present)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + 0.5) / (2 + 2 * 0.5),          #\"spam\"  (present)\n",
    "    1 - (2 + 0.5) / (2 + 2 * 0.5),      #\"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (2 + 2 * 0.5),      #\"rules\" (not present)\n",
    "    (1 + 0.5) / (2 + 2 * 0.5)           #\"hello\" (present)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "#Should be about 0.83\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using Our Model (STUDY THIS AND UNDERSTAND COMPLETELY)\n",
    "'''\n",
    "    A popular dataset is the SpamAssassin public corpus. We'll look at the files prefixed with 20021010\n",
    "'''\n",
    "\n",
    "from io import BytesIO  #So we can trat bytes as a file\n",
    "import requests         #To download the files, which\n",
    "import tarfile          #are in .tar.bz format\n",
    "\n",
    "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\"\n",
    "FILES = [\"20021010_easy_ham.tar.bz2\",\n",
    "            \"20021010_hard_ham.tar.bz2\",\n",
    "            \"20021010_spam.tar.bz2\"]\n",
    "\n",
    "OUTPUT_DIR = 'spam_data'\n",
    "\n",
    "for filename in FILES:\n",
    "    #Use requests to get the file contents at each URL\n",
    "    content = requests.get(f\"{BASE_URL}/{filename}\").content\n",
    "\n",
    "    #Wrap the in-memory bytes so we can use them as a \"file\"\n",
    "    fin = BytesIO(content)\n",
    "\n",
    "    #And extract all the files to the specified output dir\n",
    "    with tarfile.open(fileobj=fin, mode=\"r:bz2\") as tf:\n",
    "        tf.extractall(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Returning Filenames: 100%|██████████| 3302/3302 [00:00<00:00, 9019.82it/s]\n",
      "Training: 100%|██████████| 2475/2475 [00:34<00:00, 72.76it/s]\n",
      "Predictions: 100%|██████████| 825/825 [00:04<00:00, 188.43it/s]\n",
      "Confusion Matrix: 100%|██████████| 825/825 [00:00<?, ?it/s]\n",
      "\n",
      " Counter({(False, False): 666, (True, True): 100, (False, True): 33, (True, False): 26})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Each folder contains many emails, each contained in a single file. To keep things really simple, we'll just look at the subject lines of each email\n",
    "    How do we identify the subject line? When we look through the files, they all seem to start with \"Subject:\". So we'll look for that:\n",
    "'''\n",
    "\n",
    "import glob, re\n",
    "\n",
    "#modify the path to wherever you've put the files\n",
    "path = 'spam_data/*/*'\n",
    "\n",
    "data: List[Message] = []\n",
    "\n",
    "#glob.glob returns every filename that matches the wildcarded path\n",
    "\n",
    "#trying to use tqdm on my own after seeing that it works\n",
    "\n",
    "for filename in tqdm.tqdm(glob.glob(path), desc=\"Returning Filenames\"):\n",
    "    is_spam = \"ham\" not in filename\n",
    "\n",
    "    #There are some garbage characters in the emails; the errors='ignore'\n",
    "    #skips them instead of raising an exception\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break   #done with this file\n",
    "\n",
    "'''\n",
    "    Now we can split the data into training data and test data, and then we're ready to build a classifier\n",
    "'''\n",
    "\n",
    "import random\n",
    "from scratch.machine_learning import split_data\n",
    "\n",
    "random.seed(0)      #just to get same answer as book\n",
    "train_messages, test_messages = split_data(data, 0.75)\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "for _ in tqdm.tqdm(train_messages, desc=\"Training\"):\n",
    "    model.train(train_messages)\n",
    "\n",
    "#Let's generate some predictions and check how our model does:\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "predictions = [(message, model.predict(message.text)) \n",
    "                for message in tqdm.tqdm(test_messages, desc=\"Predictions\")]\n",
    "\n",
    "#Assume that spam_probability > 0.5 corresponds to spam predictions\n",
    "#and count the combinations of (actual is_spam, predicted is_spam)\n",
    "\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n",
    "                            for message, spam_probability in tqdm.tqdm(predictions, desc=\"Confusion Matrix\"))\n",
    "\n",
    "print(\"\\n\\n\", confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spammiest_words ['assistance', '95', 'attn', 'clearance', 'money', 'per', 'sale', 'rates', 'systemworks', 'adv']\nhammiest_words ['spambayes', 'users', 'razor', 'zzzzteana', 'sadev', 'apt', 'perl', 'ouch', 'spamassassin', 'bliss']\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
    "    #We probably shouldn't call private methods, but it's for a good cause\n",
    "    prob_if_spam, probs_if_ham = model._probabilities(token)\n",
    "\n",
    "    return prob_if_spam / (prob_if_spam + probs_if_ham)\n",
    "\n",
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n",
    "\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "print(\"hammiest_words\", words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    How could we get better performance? Once obvious way would be to get more data to train on. There are a number of ways to improve the model as well. Here are some possibilities that you might try:\n",
    "        -Look at the message content, not just the subject line. You'll have to be careful how to deal with th emessage headers.\n",
    "        -Our classifier takes into account every work that appears in the training set, even words that appear only once. Modify the classifier to accpet an optional min_count threshold and ignore tokens that don't appear at least that many times\n",
    "        -The tokenizer has no notion of similar words(e.g. cheap and cheapest). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. For example, a really simple stemmer function might be:\n",
    "'''\n",
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)\n",
    "\n",
    "#             Creating a good stemmer function is hard. People frequently use the Porter stemmer\n",
    "#      -Although our features are all of the form \"message contains word wi\", there's no reason why this has to be the case. In our implementation, we could add extra features like \"message contains a number\" by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}